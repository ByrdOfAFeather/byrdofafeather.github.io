<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-P5SKFSJFL1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-P5SKFSJFL1');
    </script>
    <link type="text/css"
          href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.5/css/bulma.min.css" rel="stylesheet">
    <link type="text/css"
          href="style.css" rel="stylesheet">
    <script src="src.js" type="text/javascript"></script>
    <script
            src="https://code.jquery.com/jquery-3.4.1.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research</title>
</head>
<body>
<div class="container" style="height: 100%">
    <div class="columns is-centered is-vcentered is-multiline" style='margin: 5px 5px 5px 5px'>
        <div class="column is-12">
            <h1 class="title">Research and Engineering</h1>
        </div>
        <div class="column">
            <p>As a researcher, I <a class="styled_link" href="https://github.com/byrdofafeather/ResearchTestingBed">implemented</a> a seq2seq question generation model, explored various ways of <a class="styled_link" href="https://github.com/ByrdOfAFeather/eduTech/blob/4a26939980e6457628fc2025c4a6b32897974860/irt/irt.ipynb">implementing</a> item response theory,
                and preformed my own <a class="styled_link" href="https://aclanthology.org/2022.acl-short.15/">research</a> on how syntactic and semantic features can predict how difficult a question will be to solve.
                I also <a class="styled_link" href="https://github.com/ByrdOfAFeather/pred_essay_quality">explored</a> various practical aspects of Natural Language Processing, including how to fine-tune large language models for down-stream classification tasks. As part of this
                I familiarized myself with SOTA mechanisms to interpret predictions from these large language models, including integrated gradients and LIME. I extended the most popular implementation of LIME and opened a <a href="https://github.com/marcotcr/lime/pull/687" class="styled_link">pull-request</a>, which
                adds the ability to use a T5 model to fill in blanks in data that LIME creates. <br></br>I'm currently exploring ways to use natural language feedback to improve models, partly inspiring my <a href="https://github.com/ByrdOfAFeather/fast-weights" class="styled_link">implementation</a>
                of <a href="https://mediatum.ub.tum.de/doc/814768/file.pdf" class="styled_link">fast-weights</a>, in which one model (the slow-network) learns weight updates for another (the fast-network). As well, I've taken up studying a classification task in which the goal to is determine the severity of
                medical errors based on a description of the event.</p>
                <br>
                <i>Notice: This page is still under construction</i>
        </div>
    </div>
</body>
</html>